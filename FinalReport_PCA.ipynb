{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9376848e-3fb1-4eed-b13e-c03dc8dcbb40",
   "metadata": {},
   "source": [
    "# PCA for Final Report\n",
    "\n",
    "### Team 2: Vita Khan, Quinn Reilly, Aarushi Attray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b590fd7f-929e-4d7e-a081-16697f6f1249",
   "metadata": {},
   "source": [
    "This file contains the Principal component analysis code needed to proceed with the final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6cfa592-3b24-497e-a433-0a07d5a69dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vitakhan/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Importing all useful libraries (for webscraping and ML as well)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae7ff2b4-c903-4a09-aadc-e754d659dbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the dataset\n",
    "def clean_data(df, date_prefix):\n",
    "    \"\"\"\n",
    "    Cleans the dataframe by performing various transformations and removing invalid data.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the raw data.\n",
    "        date_prefix (str): The date prefix (e.g., \"2024-01-\" for February, \"2024-07-\" for July).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    # Handling missing values through replacement \n",
    "    df.replace({\"n/a\": pd.NA, \"\\n\": pd.NA, r\"\\N\": pd.NA}, inplace=True)\n",
    "    # Convert 'started_at' and 'ended_at' to datetime format\n",
    "    df['started_at'] = pd.to_datetime(df['started_at'], errors='coerce')\n",
    "    df['ended_at'] = pd.to_datetime(df['ended_at'], errors='coerce')\n",
    "    # Filter the rows where 'started_at' or 'ended_at' starts with the specified prefix (only want January and July)\n",
    "    df = df[df['started_at'].dt.strftime('%Y-%m-').str.startswith(date_prefix)]\n",
    "    df = df[df['ended_at'].dt.strftime('%Y-%m-').str.startswith(date_prefix)]\n",
    "    # Calculating trip duration in minutes\n",
    "    df['tripduration'] = (df['ended_at'] - df['started_at']).dt.total_seconds() / 60\n",
    "    # Removing rows with invalid or zero trip durations\n",
    "    df = df[df['tripduration'] > 0]\n",
    "    # Removing rows with missing critical values\n",
    "    required_columns = [\n",
    "        'ride_id', 'rideable_type', 'started_at', 'ended_at', \n",
    "        'start_station_name', 'end_station_name', \n",
    "        'start_lat', 'start_lng', 'end_lat', 'end_lng', \n",
    "        'member_casual'\n",
    "    ]\n",
    "    df.dropna(subset=required_columns, inplace=True)\n",
    "    # Removing unrealistic trip durations (ex. >1000 minutes)\n",
    "    df = df[df['tripduration'] < 1000]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "665d7aa7-bb03-41e4-b109-603f5eb04743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://s3.amazonaws.com/hubway-data/202401-bluebikes-tripdata.zip...\n",
      "Processing 202401-bluebikes-tripdata.csv...\n",
      "Saved cleaned January data to csv_files/cleaned_jan_202401-bluebikes-tripdata.csv.\n",
      "Downloading https://s3.amazonaws.com/hubway-data/202407-bluebikes-tripdata.zip...\n",
      "Processing 202407-bluebikes-tripdata.csv...\n",
      "Saved cleaned July data to csv_files/cleaned_july_202407-bluebikes-tripdata.csv.\n"
     ]
    }
   ],
   "source": [
    "# Webscraping \n",
    "# Base URL hosting the zip files\n",
    "url = 'https://s3.amazonaws.com/hubway-data'\n",
    "\n",
    "# Fetching the list of available files\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Filtering for the specific zip files (the two we want to use)\n",
    "zip_links = []\n",
    "for download_link in soup.find_all('key'):\n",
    "    zip_file_name = download_link.text\n",
    "    if zip_file_name.endswith('.zip') and zip_file_name in ['202401-bluebikes-tripdata.zip', '202407-bluebikes-tripdata.zip']:\n",
    "        zip_links.append(f\"{url}/{zip_file_name}\")\n",
    "\n",
    "# Creating the directory for CSV files \n",
    "os.makedirs('csv_files', exist_ok=True)\n",
    "feb_csv_path = None\n",
    "aug_csv_path = None\n",
    "\n",
    "# Process each zip file\n",
    "for zip_url in zip_links:\n",
    "    try:\n",
    "        # Download the ZIP file\n",
    "        print(f\"Downloading {zip_url}...\")\n",
    "        zip_response = requests.get(zip_url)\n",
    "        zip_response.raise_for_status()\n",
    "        \n",
    "        with ZipFile(BytesIO(zip_response.content)) as zip_file:\n",
    "            for zip_info in zip_file.infolist():\n",
    "                # Skip system or hidden files\n",
    "                if \"__MACOSX\" in zip_info.filename or zip_info.filename.startswith('.'):\n",
    "                    continue\n",
    "                \n",
    "                # Process CSV files only\n",
    "                if zip_info.filename.endswith('.csv'):\n",
    "                    try:\n",
    "                        with zip_file.open(zip_info) as extracted_file:\n",
    "                            df = pd.read_csv(extracted_file)\n",
    "                            print(f\"Processing {zip_info.filename}...\")\n",
    "\n",
    "                            # Cleaning the data using func above \n",
    "                            if '202401' in zip_info.filename:\n",
    "                                df = clean_data(df, \"2024-01-\")\n",
    "                                jan_csv_path = f\"csv_files/cleaned_jan_{zip_info.filename}\"\n",
    "                                df.to_csv(jan_csv_path, index=False)\n",
    "                                print(f\"Saved cleaned January data to {jan_csv_path}.\")\n",
    "                            elif '202407' in zip_info.filename:\n",
    "                                df = clean_data(df, \"2024-07-\")\n",
    "                                july_csv_path = f\"csv_files/cleaned_july_{zip_info.filename}\"\n",
    "                                df.to_csv(july_csv_path, index=False)\n",
    "                                print(f\"Saved cleaned July data to {july_csv_path}.\")\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing file {zip_info.filename}: {e}\")\n",
    "                        continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading or processing {zip_url}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67aaad56-cc9a-47b0-bfde-779f7b528a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of cleaned January data (csv_files/cleaned_jan_202401-bluebikes-tripdata.csv):\n",
      "            ride_id  rideable_type           started_at             ended_at  \\\n",
      "0  D2F4A4783B230A84  electric_bike  2024-01-31 12:16:49  2024-01-31 12:21:02   \n",
      "1  D305CEFFD4558633   classic_bike  2024-01-12 08:14:16  2024-01-12 08:19:48   \n",
      "2  02009BB4EBA0D1F6  electric_bike  2024-01-29 15:00:05  2024-01-29 15:05:47   \n",
      "3  04C230C1C39071F7   classic_bike  2024-01-09 16:33:40  2024-01-09 17:00:41   \n",
      "4  CEAFE67E28B43852   classic_bike  2024-01-23 10:19:21  2024-01-23 10:31:39   \n",
      "\n",
      "   start_station_name start_station_id  \\\n",
      "0  Ames St at Main St           M32037   \n",
      "1  Ames St at Main St           M32037   \n",
      "2  One Memorial Drive           M32053   \n",
      "3  Ames St at Main St           M32037   \n",
      "4  Mass Ave T Station           C32063   \n",
      "\n",
      "                          end_station_name end_station_id  start_lat  \\\n",
      "0    Central Square at Mass Ave / Essex St         M32011  42.362357   \n",
      "1    Central Square at Mass Ave / Essex St         M32011  42.362500   \n",
      "2  Kennedy-Longfellow School 158 Spring St         M32065  42.361697   \n",
      "3                      Brookline Town Hall         K32005  42.362500   \n",
      "4                         Chinatown T Stop         D32019  42.341356   \n",
      "\n",
      "   start_lng    end_lat    end_lng member_casual  tripduration  \n",
      "0 -71.088163  42.365070 -71.103100        member      4.216667  \n",
      "1 -71.088220  42.365070 -71.103100        member      5.533333  \n",
      "2 -71.080273  42.369553 -71.085790        member      5.700000  \n",
      "3 -71.088220  42.333765 -71.120464        member     27.016667  \n",
      "4 -71.083370  42.352409 -71.062679        member     12.300000  \n"
     ]
    }
   ],
   "source": [
    "# Print first 50 rows of cleaned January CSV\n",
    "if jan_csv_path:\n",
    "    print(f\"Head of cleaned January data ({jan_csv_path}):\")\n",
    "    print(pd.read_csv(jan_csv_path).head())\n",
    "else:\n",
    "    print(\"\\nNo cleaned January data available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b536c36-16e9-49c8-95ed-214c30cf8525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of cleaned July data (csv_files/cleaned_july_202407-bluebikes-tripdata.csv):\n",
      "            ride_id  rideable_type               started_at  \\\n",
      "0  5FA2A0E02EC53028   classic_bike  2024-07-11 02:07:46.443   \n",
      "1  5D49B9B78C826FD5   classic_bike  2024-07-19 14:07:03.620   \n",
      "2  5FEF6FE539C078FB  electric_bike  2024-07-02 09:06:08.296   \n",
      "3  709B58276144026B   classic_bike  2024-07-30 13:55:16.971   \n",
      "4  15755F52835908F5   classic_bike  2024-07-18 19:24:45.546   \n",
      "\n",
      "                  ended_at                      start_station_name  \\\n",
      "0  2024-07-11 02:20:34.180            Maverick St at Massport Path   \n",
      "1  2024-07-19 14:08:06.530       NCAAA - Walnut Ave at Crawford St   \n",
      "2  2024-07-02 09:12:52.886  Medford Sq - Riverside Ave at River St   \n",
      "3  2024-07-30 14:54:55.077       NCAAA - Walnut Ave at Crawford St   \n",
      "4  2024-07-18 19:42:54.574            Maverick St at Massport Path   \n",
      "\n",
      "  start_station_id                     end_station_name end_station_id  \\\n",
      "0           A32044         Maverick St at Massport Path         A32044   \n",
      "1           B32027    NCAAA - Walnut Ave at Crawford St         B32027   \n",
      "2           F32002     Tufts Sq - Main St at Medford St         F32003   \n",
      "3           B32027  Harvard Square at Mass Ave/ Dunster         M32018   \n",
      "4           A32044            Addison St at Saratoga St         A32054   \n",
      "\n",
      "   start_lat  start_lng    end_lat    end_lng member_casual  tripduration  \n",
      "0  42.367741 -71.033360  42.367741 -71.033360        member     12.795617  \n",
      "1  42.316902 -71.091946  42.316902 -71.091946        member      1.048500  \n",
      "2  42.417680 -71.108055  42.401697 -71.106128        member      6.743167  \n",
      "3  42.316902 -71.091946  42.373268 -71.118579        casual     59.635100  \n",
      "4  42.367741 -71.033360  42.385181 -71.015137        casual     18.150467  \n"
     ]
    }
   ],
   "source": [
    "# Print first 50 rows of cleaned July CSV\n",
    "if july_csv_path:\n",
    "    print(f\"Head of cleaned July data ({july_csv_path}):\")\n",
    "    print(pd.read_csv(july_csv_path).head())\n",
    "else:\n",
    "    print(\"\\nNo cleaned July data available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c2b53fe-a492-4936-8184-a1133609f5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import probplot, shapiro\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92b3c272-1ed2-43ac-ab7c-f0b75fa95910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the data\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the data by cleaning the necessary columns and creating derived features.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.df): dataframe to preprocess\n",
    "    \n",
    "    Returns:\n",
    "        df (pd.df): fully preprocessed and organized data to analyze for ML\n",
    "    \"\"\"\n",
    "    required_columns = ['start_lat', 'start_lng', 'end_lat', 'end_lng', 'tripduration', 'member_casual', 'started_at']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing columns: {', '.join(missing_columns)}\")\n",
    "    \n",
    "    # Ensure numeric columns are of proper type\n",
    "    df[['start_lat', 'start_lng', 'end_lat', 'end_lng', 'tripduration']] = df[\n",
    "        ['start_lat', 'start_lng', 'end_lat', 'end_lng', 'tripduration']\n",
    "    ].apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Handle time-based features\n",
    "    df['started_at'] = pd.to_datetime(df['started_at'], errors='coerce')\n",
    "    df['hour'] = df['started_at'].dt.hour\n",
    "    df.dropna(subset=['start_lat', 'start_lng', 'end_lat', 'end_lng', 'tripduration', 'hour'], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169babe1-b622-4e6f-a2ee-09db0aa73b34",
   "metadata": {},
   "source": [
    "## PCA\n",
    "Recognizing these challenges, we incorporated PCA as our final ML model to refine our approach.  Performing PCA is helpful when dealing with multicollinearity, as it allows us to keep as much information about the X features before using them to predict y or tripduration. By applying PCA before rerunning our regression analysis, we retained the maximum variance in the data while simplifying the relationships among predictors. For this revised analysis, we focused on key features, including time of day (hour), member_casual, and rideable_type. \n",
    "\n",
    "To enhance the regression analysis, we incorporated the PCA via sklearn library modules.  We selected three predictor variables (hour, member_casual, and rideable_type) and used tripduration as the dependent variable to predict the trip duration based on factors such as the time of day, user demographics, and the type of the ride. While PCA effectively simplified the regression model, the resulting analysis still performed poorly, as demonstrated by the unsatisfactory MSE and R² values. In conclusion, despite iterative refinements, including Polynomial Regression, interaction terms, and PCA, Linear Regression remained the most effective model in this context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ac0ca47-3090-436a-9feb-94cb28f5c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac6ad7ea-eb51-4e4b-8c8e-f480bdee588a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 604.51\n",
      "R^2 Score: 0.03\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "df = preprocess_data(df)\n",
    "df['member_casual'] = df['member_casual'].astype('category').cat.codes\n",
    "df['rideable_type'] = df['rideable_type'].astype('category').cat.codes\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df[['hour', 'member_casual','rideable_type']].values\n",
    "y = df['tripduration']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2) \n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Perform regression on principal components\n",
    "model = LinearRegression()\n",
    "model.fit(X_pca, y)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = model.predict(X_pca)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R^2 Score: {r2:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
